# Design

Aus den vorliegenden Daten über die Ergebnisse von INLOOM, sowie den manuellen 
Korrekturen muss eine Datenstruktur abgeleitet werden, welche das Sammeln, die 
einfache Darstellung, sowie einen Vergleich zwischen manEval und autoEval 
zulässt. Die Datenstruktur spielt an folgenden Stellen eine essentielle Rolle:

* Speicherung der Daten in der Application Database 
* Direkter Vergleich von EINER manEval mit EINER autoEval
* Aggregation von Eval Daten zur statistischen Auswertung
* Darstellung von vorliegenden Daten und den erfassten Statistiken

## Klassen

![Entity/Class Diagram][entities]

### TestDataSet
Die grundlegendste Einheit in INLOOM QT besteht aus einem Vergleiche von zwei
Evaluationen. Ohne ein *Paar* von Evaluationen, die miteinander verglichen werden 
sollen, macht offensichtlich kein anderer Aspekt der Software Sinn. Ein 
TestDataSet verbindet also *einen* IST-Datensatz mit *einem* SOLL-Datensatz und 
entspricht damit *einem* Test Case. So ein Test Case wird repräsentiert von einem 
``TestDataSet``. Alle Eigenschaften, die die beiden Evaluationen teilen, können 
im TestDataSet persistiert werden. 

In einem TestCase sollen jeweils die manuelle und automatische Korrektur der 
gleichen studentischen Lösung, mit einander verglichen werden. Die 
**Id der studentischen Lösung identifiziert** also auch das 
**TestDataSet eindeutig**, da jede studentische Lösung einzigartig ist. Die Id 
einer studentischen Lösung sollte sich zusammensetzen aus der ``Aufgaben-Id`` und der 
``Studenten-Id``. In den INLOOM Resultaten werden diese beiden Ids in der TestModelId 
zusammengefasst. Ich schlage allerdings vor, die Ids wieder aufzuspalten. So 
würde die Aufgaben-Id einzeln vorliegen und könnte zur Strukturierung des 
Frontends verwendet werden.

> #### **QUESTION**
> Ist das richtig? Brauche ich MCS Version & Id um Testdaten verschiedener Versionen
> zu verwalten?

### AutoEval
Eine ``AutoEval`` entspricht einem von INLOOM generierten Result XML File und soll
die XML Dateien möglichst vollständig abbilden. Die Testdatensätze drohen in 
nächster Zeit nicht so groß zu werden, dass wir uns Sorgen über die Größe des 
Datensatze machen müssten, also brauchen wir keine MetaDaten wegschmeißen.
Eine ``AutoEval`` kann anhand eines Keys aus (``exercise_id``, ``expert_solution_id``,
``student_id``) identifiziert werden. Die von INLOOM generierten Results werden im 
Parameter ``constraint_results`` einer AutoEval persistiert. Außerdem hat die 
automatische Evaluation im Gegensatz zur manuell erstellten die beiden Metadaten-Attribute
``mcs_identifier`` sowie ``mcs_version``.

### ManEval
Eine ``ManEval`` entspricht einer durch einen Instruktor angefertigten Evaluation
einer studentischen Lösung. Bei der manuellen Korrektur verwendet der Instruktor
eine Liste notwendiger Kriterien (Features). Für jede erwartete Eigenschaft der 
Lösung vergibt er Punkte. Diese Feature Bewertungen, werden von IQT in einer Liste von 
Results festgehalten. Jeder Bewerter hat einen eignen Stil, korrigiert streng oder
weniger streng. Was für einen großen Einfluss unterschiedliche Bewerter auf das 
Endergebnis haben kann, wurde hinreichend diskutiert. Deshalb ist es sinnvoll in
den manuellen Korrekturen zu persistieren, wer der jeweilige Korrektor war. Zu diesem
Zweck besitzt jede ManEval die Variable ``evaluator_id``.

### Evaluation
The ``Evaluation`` is the common parent class of both the Man- and the AutoEval.
Es existiert eine Evaluation für jede Man- und AutoEval. In der Evaluation werden
diejenigen Attribute einer Eval persistiert, die sowohl in der automatischen als 
auch in der manuellen Evaluation vorkommen.


### ManEval


## Workflows

### Adding Test Cases
Die Aussagekraft von Tests hängt in hohem Maße vom Umfang der vorgenommenen Tests ab.
Bei Systemen wie INLOOM, zu denen der User direkten Kontakt hat, wäre ein unentdeckter
Bug äußerst peinlich und würde wahrscheinlich einen schlechten Eindruck hinterlassen.
Deshalb sollte die Test Abdeckung besonders hoch sein sollte. Das Problem in diesem Fall ist, 
dass es besonders schwierig ist Testdaten zu erfassen, da manuelle Korrekturen als Pen and Paper 
Aufzeichnung eines Diagrams, sowie manuellen Annotationen zu besagtem Diagram, vorliegen. Diese 
Daten müssen digitalisiert werden, um einen Testdatensatz zu gewinnen. Der 
Digitalisierungsprozess ist der aufwändigste Teil der Testerstellung und muss deshalb so gut 
wie möglich durch die zu entwickelnde Software unterstützt werden. Offensichtlich können die 
bereits in digitaler Form vorliegenden Resultate von INLOOM automatisch transformiert werden.

![Adding Test Cases][add_test_data_workflow]

## Page Tree
![Page Tree][pagetree]

[//]: # (LINKS)

[//]: # (IMAGES)
[add_test_data_workflow]: ../images/graphics/AddingTestCasesWorkflow.png
[entities]: ../images/graphics/Entities.png
[pagetree]: ../images/graphics/PageTree.png
