@thesis{1,
  title       = {Automatic Feedback for UML Modeling Exercises as an Extension of INLOOP},
  author      = {Markus Hamann},
  year        = {2020},
  institution = {TU Dresden}
}

@online{2,
  title      = {The {COVID}-19 pandemic has changed education forever. This is how},
  url        = {https://www.weforum.org/agenda/2020/04/coronavirus-education-global-covid19-online-digital-learning/},
  abstract   = {Research suggests that online learning has been shown to increase retention
  of information, and take less time, meaning the changes coronavirus have caused might be
  here to stay.},
  titleaddon = {World Economic Forum},
  urldate    = {2020-12-03},
  langid     = {english},
  keywords   = {Motivation, e-learning}
}

@online{3,
  title      = {Zahl der Studierenden erreicht im Wintersemester 2019/2020 neuen Höchststand},
  url        = {https://www.destatis.de/DE/Presse/Pressemitteilungen/2019/11/PD19_453_213.html},
  abstract   = {Der Zulauf an die deutschen  setzt sich fort: Im Wintersemester 2019/2020 sind
  nach ersten vorläufigen Ergebnissen des Statistischen Bundesamtes (Destatis) 2 897 300 
  Studentinnen und Studenten an einer Hochschule in Deutschland eingeschrieben. Damit erhöhte
  sich die Zahl der Studierenden im Vergleich zum Wintersemester 2018/2019 um 29 100 (+1,0 \%).
  Im Zehnjahresvergleich sind damit aktuell rund 37 \% mehr Studierende an den deutschen 
  Hochschulen immatrikuliert (Wintersemester 2009/2010: 2 121 200 Studierende). Einen Rückgang
  der Studierendenzahl in einem Wintersemester hatte es zuletzt 2007/2008 gegeben. Die Zahl
  der Studienanfängerinnen und -anfänger ist dagegen im Studienjahr 2019 zum zweiten Mal
  in Folge gesunken.},
  titleaddon = {Statistisches Bundesamt},
  urldate    = {2020-11-24},
  langid     = {german},
  keywords   = {Motivation}
}

@article{4,
  title        = {Higher Ed Needs a Long-Term Plan for Virtual Learning},
  issn         = {0017-8012},
  url          = {https://hbr.org/2020/05/higher-ed-needs-a-long-term-plan-for-virtual-learning},
  abstract     = {How to move beyond the immediate response to Covid-19.},
  journaltitle = {Harvard Business Review},
  author       = {{DeVaney}, James and Shimshon, Gideon and Rascoff, Matthew and Maggioncalda, Jeff},
  urldate      = {2020-12-03},
  date         = {2020-05-05},
  note         = {Section: Education},
  keywords     = {Education, Innovation, Motivation, Risk management}
}

@online{5,
  title      = {INLOOP: interactive learning center for object-oriented programming},
  url        = {https://github.com/st-tu-dresden/inloop},
  titleaddon = {Github},
  urldate    = {2020-12-20}
}

@book{6,
  title    = {Automated Grading of Class Diagrams},
  abstract = {Drawing {UML} diagrams, such as class diagrams, 
  is an essential learning task in many software engineering courses. 
  In course assignments, students are tasked to draw models that 
  describe scenarios, model requirements, or system designs. The 
  course instructor usually grades the diagrams manually by comparing
  a student's solution model with a template solution model made by 
  the instructor. However, modelling is not an exact science, and 
  multiple correct solutions or variants may exist. This makes grading
   {UML} assignments a cumbersome task, especially when there are many 
   assignments to grade. Therefore, there is a need for an automated 
   grading tool that aids the instructor in the grading process. This 
   paper presents an approach for automated grading of {UML} class 
   diagrams. We propose a metamodel that establishes mappings between 
   the instructor solution and all the solutions for a class. The 
   approach uses a grading algorithm that uses syntactic, semantic 
   and structural matching to match a student's solutions with the 
   template solution. We evaluate the algorithm on a real assignment 
   for modeling a Flight Ticketing domain model for a class of 20 
   students and report our findings.},
  author   = {Bian, Weiyi and Alam, Omar and Kienzle, Jörg},
  date     = {2019-09-11},
  doi      = {10.1109/MODELS-C.2019.00106}
}

@inproceedings{7,
  location   = {New York, {NY}, {USA}},
  title      = {Is automated grading of models effective? assessing automated grading of class 
  diagrams},
  isbn       = {978-1-4503-7019-6},
  url        = {https://doi.org/10.1145/3365438.3410944},
  doi        = {10.1145/3365438.3410944},
  series     = {{MODELS} '20},
  shorttitle = {Is automated grading of models effective?},
  abstract   = {Learning how to model the structural properties of a problem domain or an 
  object-oriented design in the form of a class diagram is an essential learning task in many 
  software engineering courses. Since the grading of models is a time-consuming activity, 
  automated grading approaches have been developed to assist the instructor by speeding up 
  the grading process, as well as ensuring consistency and fairness for large classrooms. 
  This paper empirically evaluates the efficacy of one such automated grading approach when 
  applied in two real world settings: a beginner undergraduate class of 103 students required 
  to create an object-oriented design model, and an advanced undergraduate class of 89 
  students elaborating a domain model. The results of the experiment highlight a) the need 
  to adapt the grading strategy and strictness to the level of the students and the grading 
  style of the instructor, and b) the importance of considering multiple solution variants 
  when grading. Modifications to the grading algorithm are proposed and validated 
  experimentally.},
  pages      = {365--376},
  booktitle  = {Proceedings of the 23rd {ACM}/{IEEE} International Conference on Model Driven 
  Engineering Languages and Systems},
  publisher  = {Association for Computing Machinery},
  author     = {Bian, Weiyi and Alam, Omar and Kienzle, Jörg},
  urldate    = {2020-12-27},
  date       = {2020-10-18}
}

@article{8,
  title        = {{UMLGrader}: an automated class diagram grader},
  volume       = {27},
  issn         = {1937-4771},
  shorttitle   = {{UMLGrader}},
  abstract     = {We present {UMLGrader}, a system designed to provide automated feedback to 
  students on class diagrams written in the Unified Modeling Language ({UML}). Given a diagram 
  which is constructed to model a tightly constrained problem, the tool compares the diagram 
  against a standard solution and provides feedback on missing elements and other errors. 
  This supports using canned exercises to familiarize students with {UML} notation. We discuss 
  tool requirements, our experiences with using it in a class at the sophomore/junior level, 
  and possible future improvements.},
  pages        = {47--54},
  number       = {1},
  journaltitle = {Journal of Computing Sciences in Colleges},
  shortjournal = {J. Comput. Sci. Coll.},
  author       = {Hasker, Robert W.},
  date         = {2011-10-01},
  keywords     = {{ITS}}
}

@article{9,
  title        = {Web Based Software Modeling Exercises in Large-Scale Software Engineering 
  Courses},
  doi          = {10.1109/CSEET.2009.38},
  abstract     = {We present a web based {eLearning} system to support software modeling 
  exercises in large-scale software engineering courses. Students get the task to create 
  a domain model based one given textual specification of an application domain. They have 
  to specify the domain model by a {UML} class diagram and import it into the {eLearning} 
  system. The system verifies the student's solution and compares it with a set of 
  predetermined sample solutions. This approach is a first step towards an automatic 
  testing of object-oriented models under the conditions of university classes with a large 
  number of students. We discuss it and report about initial experiences in using the 
  {eLearning} system.},
  journaltitle = {2009 22nd Conference on Software Engineering Education and Training},
  author       = {Demuth, B. and Weigel, D.},
  date         = {2009},
  keywords     = {{ITS}}
}

@article{10,
  title        = {Supporting collaborative learning and problem-solving in a constraint-based 
  {CSCL} environment for {UML} class diagrams},
  volume       = {2},
  issn         = {1556-1615},
  url          = {https://doi.org/10.1007/s11412-007-9018-0},
  doi          = {10.1007/s11412-007-9018-0},
  abstract     = {We present {COLLECT}-{UML}, a constraint-based intelligent tutoring system 
  ({ITS}) that teaches object-oriented analysis and design using Unified Modelling Language 
  ({UML}). {UML} is easily the most popular object-oriented modelling technology in current 
  practice. While teaching how to design {UML} class diagrams, {COLLECT}-{UML} also provides 
  feedback on collaboration. Being one of constraint-based tutors, {COLLECT}-{UML} represents 
  the domain knowledge as a set of constraints. However, it is the first system to also 
  represent a higher-level skill such as collaboration using the same formalism. We started 
  by developing a single-user {ITS} that supported students in learning {UML} class diagrams. 
  The system was evaluated in a real classroom, and the results showed that students’ 
  performance increased significantly. In this paper, we present our experiences in extending 
  the system to provide support for collaboration as well as domain-level support. We describe 
  the architecture, interface and support for collaboration in the new, multi-user system. The 
  effectiveness of the system has been evaluated in two studies. In addition to improved 
  problem-solving skills, the participants both acquired declarative knowledge about effective 
  collaboration and did collaborate more effectively. The participants have enjoyed working 
  with the system and found it a valuable asset to their learning.},
  pages        = {159--190},
  number       = {2},
  journaltitle = {International Journal of Computer-Supported Collaborative Learning},
  shortjournal = {Computer Supported Learning},
  author       = {Baghaei, Nilufar and Mitrovic, Antonija and Irwin, Warwick},
  urldate      = {2020-11-27},
  date         = {2007-09-01},
  langid       = {english}
}

@inproceedings{11,
  title      = {The marking system for {CourseMaster}},
  volume     = {34},
  doi        = {10.1145/637610.544431},
  abstract   = {{CourseMaster} ({CM}) is a Computer Based Assessment ({CBA}) system. This 
  paper describes the motivation and aims for developing {CM}'s Marking System. It also 
  explains the architectural forces and design decisions that have been established in order
  to engineer the Marking System. The Marking System adheres to the rigid specifications 
  of the initial {CM}'s design, which are: reliability, coherency, security, feedbackrichness,
  extensibility and customisability. The above notions and the features that {CM}'s Marking 
  System provides are examined in detail. Concrete implementation issues are also discussed 
  with conclusions on usability and extensibility observations.},
  eventtitle = {{ACM} Sigcse Bulletin},
  pages      = {46--50},
  author     = {Higgins, Colin and Symeonidis, Pavlos and Tsintsifas, Athanasios},
  date       = {2002-09-01},
  keywords   = {{ITS}}
}

@article{12,
  title    = {{COCLAC} - Feedback Generation for Combined {UML} Class and Activity Diagram 
  Modeling Tasks},
  abstract = {We introduce a newly developed visual programming and {UML} modeling tool for 
  educational use. It’s implemented to provide students with informative feedback during 
  exercises as well as to assess the submissions of students automatically. Tasks combine 
  class and activity modeling aspects as well as some simple programming requirements. The 
  system is web-based and named {COCLAC}. Visual programming is done through an included 
  {UML} editor. Created diagrams are syntactically checked and automatically converted into 
  Java code using several conventions. Thereafter, code is executable on a server and semantic
   correctness can be checked through automatic tests.},
  pages    = {8},
  author   = {Beck, Philip-Daniel and Mahlmeister, Thomas and Iﬂand, Marianus and Puppe, Frank},
  langid   = {english},
  keywords = {{ITS}}
}

@inproceedings{13,
  title    = {A Structural Approach to Assess Graph-Based Exercises},
  isbn     = {978-3-319-27652-6},
  doi      = {10.1007/978-3-319-27653-3_18},
  abstract = {This paper proposes a structure driven approach to assess graph-based exercises.
  Given two graphs, a solution and an attempt of a student, this approach computes a mapping 
  between the node sets of both graphs that maximizes the student’s grade, as well as a 
  description of the differences between the two graph. The proposed algorithm uses heuristics
  to test the most promising mappings first and prune the remaining when it is sure that a 
  better mapping cannot be computed.
  The proposed algorithm is applicable to any type of document that can be parsed into its 
  graph-inspired data model. This data model is able to accommodate diagram languages, such as
  {UML} or {ER} diagrams, for which this kind of assessment is typically used. However, the 
  motivation for developing this algorithm is to combine it with other assessment models, such
  as the test case model used for programming language assessment.
  The proposed algorithm was validated with thousands of graphs with different features produced
  by a synthetic data generator. Several experiments were designed to analyse the impact of 
  different features such as graph size, and amount of difference between solution and attempt.},
  pages    = {182--193},
  author   = {Sousa, Rúben and Leal, José},
  date     = {2015-06-18},
  keywords = {{ITS}}
}

@article{14,
  title        = {An automatic correction tool that can learn},
  doi          = {10.1109/FIE.2011.6142766},
  abstract     = {The majority of Computer Based Assessment ({CBA}) environments have been 
  designed for fixed-response questions. This feature is not enough for the university context 
  since not all the skills can be reduced to this typology of questions and free response 
  exercises are required. In this paper, we present a web-based tool designed to automatically 
  correct exercises that require a diagram or a graph to be solved. The main novelty of this 
  tool is the strategy used for the correction. The tool starts without the knowledge about 
  correct and incorrect solutions and then learns about the solutions provided by teachers 
  and students. It automatically records the information of all entered solutions, the teacher 
  correction and the corresponding feedback in a system database. When a new student solution 
  is entered the information of the database is used for correction. The main components of 
  the tool are the graph editor module, designed to support diagram drawing, and the correction 
  module, which corrects the solutions entered by the students. These modules are integrated in 
  a more general e-learning platform denoted {ACME}. {ACME} provides teachers all 
  functionalities required for student work tracking, assessment, personalized attention, etc. 
  The proposed tool has been evaluated on experimental group.},
  journaltitle = {Proceedings - Frontiers in Education Conference},
  shortjournal = {Proceedings - Frontiers in Education Conference},
  author       = {Prados, Ferran and Soler, Josep and Boada, Imma and Poch, Jordi},
  date         = {2011-10-01},
  keywords     = {{ITS}}
}

@book{15,
  title    = {Automated Grading of Class Diagrams},
  abstract = {Drawing {UML} diagrams, such as class diagrams, is an essential learning task 
  in many software engineering courses. In course assignments, students are tasked to draw 
  models that describe scenarios, model requirements, or system designs. The course instructor 
  usually grades the diagrams manually by comparing a student's solution model with a template 
  solution model made by the instructor. However, modelling is not an exact science, and 
  multiple correct solutions or variants may exist. This makes grading {UML} assignments a 
  cumbersome task, especially when there are many assignments to grade. Therefore, there is 
  a need for an automated grading tool that aids the instructor in the grading process. This 
  paper presents an approach for automated grading of {UML} class diagrams. We propose a 
  metamodel that establishes mappings between the instructor solution and all the solutions 
  for a class. The approach uses a grading algorithm that uses syntactic, semantic and 
  structural matching to match a student's solutions with the template solution. We evaluate 
  the algorithm on a real assignment for modeling a Flight Ticketing domain model for a class 
  of 20 students and report our findings.},
  author   = {Bian, Weiyi and Alam, Omar and Kienzle, Jörg},
  date     = {2019-09-11},
  doi      = {10.1109/MODELS-C.2019.00106},
  keywords = {{ITS}}
}

@inproceedings{16,
  location  = {New York, {NY}, {USA}},
  title     = {Automated checks on {UML} diagrams},
  isbn      = {978-1-4503-0697-3},
  url       = {https://doi.org/10.1145/1999747.1999761},
  doi       = {10.1145/1999747.1999761},
  series    = {{ITiCSE} '11},
  abstract  = {Automated checks for software artefacts like {UML} diagrams used in automated 
  assessment or tutoring systems do often rely on direct comparisons between a solution and a 
  sample solution. This approach has drawbacks regarding flexibility in face of different 
  possible solutions which are quite common in modeling tasks. This paper presents an 
  alternative technique for checking {UML} class diagrams based on graph queries which promises 
  to be more flexible.},
  pages     = {38--42},
  booktitle = {Proceedings of the 16th annual joint conference on Innovation and technology 
  in computer science education},
  publisher = {Association for Computing Machinery},
  author    = {Striewe, Michael and Goedicke, Michael},
  urldate   = {2020-11-26},
  date      = {2011-06-27},
  keywords  = {{ITS}, automated tutoring, diagram analysis, intelligent tutoring systems}
}

@article{17,
  title    = {Toward the Automatic Assessment of Text Exercises},
  abstract = {Exercises are an essential part of learning. Manual assessment of exercises 
  requires efforts from instructors and can also lead to quality problems and inconsistencies 
  between assessments. Especially with growing student populations, this also leads to 
  delayed grading, and it is more and more difﬁcult to provide individual feedback.},
  pages    = {4},
  author   = {Bernius, Jan Philip and Bruegge, Bernd},
  langid   = {english},
  keywords = {{ITS}}
}

@inproceedings{18,
  title      = {Automatic Grading of Free-Form Diagrams with Label Hypernymy},
  doi        = {10.1109/LaTiCE.2013.33},
  abstract   = {The automatic grading of free-form answers is gaining importance. This paper 
  presents the only system we know of capable of automatically grading free-form diagrammatic 
  answers. Our approach identifies the parts of a diagram that carry specific meaning in that 
  domain, termed minimal meaningful units ({MMUs}), and matches them with equivalent parts in 
  a model solution. The result of the matching is used to calculate the grade and to generate 
  appropriate feedback. The matching is complicated by errors, omissions, and superfluous 
  items in the student answer. In particular, inheritance hierarchies in diagrams can cause 
  labels in the diagram to be fragmented across several objects. Our approach has been 
  successfully applied to a variety of diagram types and grades diagrams better than a human 
  marker. We briefly describe a set of tools that allow the easy creation of questions, 
  marking schemes, and diagram editors suitable for online assessment and embedding in a 
  {VLE} quiz engine.},
  eventtitle = {2013 Learning and Teaching in Computing and Engineering},
  pages      = {136--142},
  booktitle  = {2013 Learning and Teaching in Computing and Engineering},
  author     = {Smith, N. and Thomas, P. and Waugh, K.},
  date       = {2013-03}
}

@article{19,
  title    = {Diagram matching for human-computer collaborative assessment},
  url      = {/articles/conference_contribution/Diagram_matching_for_human-computer_
  collaborative_assessment/9488852/1},
  abstract = {Diagrams are an important part of many assessments. When diagrams consisting 
  of boxes joined by connectors are drawn on a computer, the resulting structures can be 
  matched against each other to determine similarity. This paper discusses ways of doing such 
  matching, and its application in the context of human-computer collaborative assessment. 
  Results show that a simple heuristic process is effective in finding similarities in such 
  diagrams. The practical usefulness of this varies in different contexts, as students often 
  produce remarkably dissimilar diagrams.},
  author   = {Tselonis, Christos and Sargeant, John and Wood, Mary {McGee}},
  urldate  = {2020-11-27},
  date     = {2005-01-01},
  langid   = {english},
  note     = {Publisher: Loughborough University}
}

@inproceedings{20,
  title      = {A Design of an Assessment System for {UML} Class Diagram},
  doi        = {10.1109/ICCSA.2007.31},
  abstract   = {The Unified Modeling Language ({UML}) is probably the most widely known and used 
  notation for object- oriented analysis and design. {UML} consists of various graphical 
  notations, which capture the static system structures (class diagrams), system component 
  behaviors (state transition diagrams) and system component interactions (collaboration and 
  sequence diagrams). {UML} notations can be produced with the help of {CASE} (Computer-aided 
  software engineering) tools such as Rational Rose. Basically, we proposed the development 
  of an Assessment system for {UML} class diagram, the {UML} Class Diagram Assessor ({UCDA}). 
  This tool will receive a students {UML} class diagram in the form of Rational Rose petal 
  files. In this paper we present a design of {UML} Class Diagram Assessor ({UCDA}) that 
  evaluates {UML} class diagrams automatically. {UCDA} evaluates the diagram based on three 
  aspects: its structure; its correctness and language used. The output of {UCDA} is a list 
  of comments on a diagram that is hoped to guide students in understanding on how to 
  represent the system requirement in {UML} model correctly.},
  eventtitle = {2007 International Conference on Computational Science and its Applications 
  ({ICCSA} 2007)},
  pages      = {539--546},
  booktitle  = {2007 International Conference on Computational Science and its Applications 
  ({ICCSA} 2007)},
  author     = {Ali, N. H. and Shukur, Z. and Idris, S.},
  date       = {2007-08}
}

@article{21,
  title        = {An automated student diagram assessment system},
  volume       = {30},
  issn         = {0097-8418},
  url          = {https://doi.org/10.1145/290320.283089},
  doi          = {10.1145/290320.283089},
  abstract     = {The teaching of systems analysis and design diagramming methods commonly 
  utilises Computer Aided Software Engineering ({CASE}) tools to provide a way for students 
  to actively practice the subject. However, many versions of these tools do not cater for 
  the academic users who will require assistance in the underlying methods as well as the 
  usage of the tool. The automated diagram comparison system developed at the University of 
  Teesside can be used by students to compare a diagram that they consider to be a solution 
  to a given problem against a model answer, and receive feedback commenting on their 
  solution, which strengthens their understanding of the subject. This paper outlines a 
  framework for such interactive learning, describes the use of the diagram comparison system, 
  and highlights the benefits for the student.},
  pages        = {122--124},
  number       = {3},
  journaltitle = {{ACM} {SIGCSE} Bulletin},
  shortjournal = {{SIGCSE} Bull.},
  author       = {Hoggarth, Gil and Lockyer, Mike},
  urldate      = {2020-12-28},
  date         = {1998-08-01}
}

@inproceedings{22,
  location   = {Baltimore Maryland {USA}},
  title      = {{ArTEMiS}: An Automatic Assessment Management System for Interactive Learning},
  isbn       = {978-1-4503-5103-4},
  url        = {https://dl.acm.org/doi/10.1145/3159450.3159602},
  doi        = {10.1145/3159450.3159602},
  shorttitle = {{ArTEMiS}},
  abstract   = {The increasing number of students in computer science courses leads to high 
  efforts in manual assessment of exercises. Existing assessment systems are not designed for 
  exercises with immediate feedback in large classes. In this paper, we present an {AuTomated} 
  {assEssment} Management System for interactive learning.},
  eventtitle = {{SIGCSE} '18: The 49th {ACM} Technical Symposium on Computer Science Education},
  pages      = {284--289},
  booktitle  = {Proceedings of the 49th {ACM} Technical Symposium on Computer Science Education},
  publisher  = {{ACM}},
  author     = {Krusche, Stephan and Seitz, Andreas},
  urldate    = {2020-11-27},
  date       = {2018-02-21},
  langid     = {english}
}

@inproceedings{23,
  title    = {Teaching {UML} Skills to Novice Programmers Using a Sample Solution Based 
  Intelligent Tutoring System},
  abstract = {Modeling skills are essential during the process of learning programming. {ITS} 
  systems for modeling are typically hard to build due to the ill-definedness of most modeling 
  tasks. This paper presents a system that can teach {UML} skills to novice programmers. The 
  system is "simple and cheap" in the sense that it only requires an expert solution against 
  which the student solutions are compared, but still flexible enough to accommodate certain 
  degrees of solution flexibility and variability that are characteristic of modeling tasks. 
  An empirical evaluation via a controlled lab study showed that the system worked fine and, 
  while not leading to significant learning gains as compared to a control condition, still 
  revealed some promising results.},
  author   = {Schramm, Joachim and Strickroth, Sven and Le, Nguyen-Thinh and Pinkwart, Niels},
  date     = {2012-05-25},
  keywords = {{ITS}},
  file     = {Full Text PDF:C\:\\Users\\darkf\\Zotero\\storage\\BSKEDAYF\\Schramm et al. - 2012 - 
  Teaching UML Skills to Novice Programmers Using a .pdf:application/pdf}
}

@article{24,
  title    = {A Constraint-based Assessment Approach for Free Form Design of Class Diagrams 
  using {UML}},
  abstract = {For a design problem in a modeling language like {UML}, there is no single 
  correct solution. Usually, there are many solutions, which satisfy a given problem 
  specification. In principle, the solution space can be infinite. However, current approaches
  evaluate student's entries by comparing them with a limited set of possible solutions and 
  errors. Some other approaches anticipate design decisions by providing students with a set 
  of appropriate design elements to select, thus ignoring the learning objectives of mastering 
  the object-oriented analysis and design. We are extending the {ArgoUML}, an {UML} tool, to a 
  learning system, which enables students to design a class diagram using {UML} in free-form. 
  The core component of this system is an assessment module, which evaluates a class diagram 
  based on design guidelines. We apply the constraint-based approach to model a solution space 
  for the given use case, which represents a design problem. This paper describes our 
  assessment approach and the current stage of our system which is able to evaluate elements 
  of a class diagram: classes, associations and the multiplicity of associations.},
  author   = {Le, Nguyen-Thinh},
  date     = {2020-11-27},
  keywords = {{ITS}}
}

@inproceedings{25,
  location  = {New York, {NY}, {USA}},
  title     = {Automatic assessment of students' software models using a simple heuristic 
  and machine learning},
  isbn      = {978-1-4503-8135-2},
  url       = {https://doi.org/10.1145/3417990.3418741},
  doi       = {10.1145/3417990.3418741},
  series    = {{MODELS} '20},
  abstract  = {Software models are increasingly popular. To educate the next generation of 
  software engineers, it is important that they learn how to model software systems well, 
  so that they can design them effectively in industry. It is also important that instructors 
  have the tools that can help them assess students' models more effectively. In this paper,
  we investigate how a tool that combines a simple heuristic with machine learning techniques 
  can be used to help assess student submissions in model-driven engineering courses. We apply 
  our proposed technique to first identify submissions of high quality and second to predict 
  approximate letter grades. The results are comparable to human grading and a complex 
  rule-based technique for the former and surprisingly accurate for the latter.},
  pages     = {1--10},
  booktitle = {Proceedings of the 23rd {ACM}/{IEEE} International Conference on Model Driven 
  Engineering Languages and Systems: Companion Proceedings},
  publisher = {Association for Computing Machinery},
  author    = {Boubekeur, Younes and Mussbacher, Gunter and {McIntosh}, Shane},
  urldate   = {2020-12-27},
  date      = {2020-10-16},
  keywords  = {assessment, domain modeling, grading, heuristics, {ITS}, Umple}
}

@inproceedings{26,
  title      = {Formative computer based assessment in diagram based domains},
  volume     = {38},
  doi        = {10.1145/1140124.1140152},
  abstract   = {This research argues that the formative assessment of student coursework in 
  free-form, diagram-based domains can be automated using {CBA} techniques in a way which is 
  both feasible and useful. Formative assessment is that form of assessment in which the 
  objective is to assist the process of learning undertaken by the student. The primary 
  deliverable associated with formative assessment is feedback. {CBA} courseware provides 
  facilities to implement the full lifecycle of an exercise through an integrated, online 
  system. This research demonstrates that {CBA} offers unique opportunities for student 
  learning through formative assessment, including allowing students to correct their 
  solutions over a larger number of submissions than it would be feasible to allow within 
  the context of traditional assessment forms.
The approach to research involves two main phases. The first phase involves designing and 
implementing an assessment course using the {CourseMarker} / {DATsys} {CBA} system. This system, in common with may other examples of {CBA} courseware, was intended primarily to conduct summative assessment. The benefits and limitations of the system are identified. The second phase identifies three extensions to the architecture which encapsulate the difference in requirements between summative assessment and formative assessment, presents a design for the extensions, documents their implementation as extensions to the {CourseMarker} / {DATsys} architecture and evaluates their contribution.
The three extensions are novel extensions for free-form {CBA} which allow the assessment 
of the aesthetic layout of student diagrams, the marking of student solutions where multiple 
model solutions are acceptable and the prioritisation and truncation of feedback prior to 
its presentation to the student.Evaluation results indicate that the student learning process 
can be assisted through formative assessment which is automated using {CBA} courseware. 
The students learn through an iterative process in which feedback upon a submitted student 
coursework solution is used by the student to improve their solution, after which they may 
re-submit and receive further feedback.},
  eventtitle = {{ACM} {SIGCSE} Bulletin},
  pages      = {98--102},
  author     = {Higgins, Colin and Bligh, Brett},
  date       = {2006-06-26}
}

@article{27,
  title        = {Automatically assessing graph-based diagrams},
  volume       = {33},
  doi          = {10.1080/17439880802324251},
  abstract     = {To date there has been very little work on the machine understanding of 
  imprecise diagrams - diagrams drawn by students in response to assessment questions. While 
  there have successful attempts at assessing text (essays) automatically, little success 
  with diagrams has been reported. In this paper, we explain an approach to the automatic 
  interpretation of graph-based diagrams based on a 5-stage framework. The paper reports on 
  the evaluation of some experiments in automatically grading student diagrams produced 
  under examination conditions which show good agreement with the performance of human 
  markers. The paper also describes how the automatic marking algorithm is being used in a 
  variety of teaching and learning tools.},
  journaltitle = {Learning Media and Technology},
  shortjournal = {Learning Media and Technology},
  author       = {Thomas, Pete and Smith, Neil and Waugh, Kevin},
  date         = {2008-09-01}
}

@online{28,
  title      = {Automatic assessment of sequence diagrams},
  url        = {/paper/Automatic-assessment-of-sequence-diagrams-Thomas-Smith/
  ef570840cbb182d6e8f861ced321992e20b94f93},
  abstract   = {In previous work we showed how student-produced entity-relationship diagrams 
  ({ERDs}) could be automatically marked with good accuracy when compared with human markers. 
  In this paper we report how effective the same techniques are when applied to syntactically 
  similar {UML} sequence diagrams and discuss some issues that arise which did not occur with 
  {ERDs}. We have found that, on a corpus of 100 student-drawn sequence diagrams, the automatic
  marking technique is more reliable that human markers. In addition, an analysis of this 
  corpus revealed significant syntax errors in student-drawn sequence diagrams. We used the 
  information obtained from the analysis to build a tool that not only detects syntax errors 
  but also provides feedback in diagrammatic form. The tool has been extended to incorporate 
  the automatic marker to provide a revision tool for learning how to model with sequence 
  diagrams.},
  titleaddon = {undefined},
  author     = {Thomas, P. and Smith, N. and Waugh, Kevin G.},
  urldate    = {2020-11-27},
  date       = {2008},
  langid     = {english}
}

@article{29,
  title        = {A Coefficient of Agreement for Nominal Scales},
  volume       = {20},
  issn         = {0013-1644},
  url          = {https://doi.org/10.1177/001316446002000104},
  doi          = {10.1177/001316446002000104},
  pages        = {37--46},
  number       = {1},
  journaltitle = {Educational and Psychological Measurement},
  shortjournal = {Educational and Psychological Measurement},
  author       = {Cohen, Jacob},
  urldate      = {2020-12-29},
  date         = {1960-04-01},
  langid       = {english},
  note         = {Publisher: {SAGE} Publications Inc}
}

  
@article{30,
  title        = {Reliability of Content Analysis: The Case of Nominal Scale Coding},
  volume       = {19},
  issn         = {0033-362X},
  url          = {https://www.jstor.org/stable/2746450},
  shorttitle   = {Reliability of Content Analysis},
  pages        = {321--325},
  number       = {3},
  journaltitle = {The Public Opinion Quarterly},
  author       = {Scott, William A.},
  urldate      = {2020-12-29},
  date         = {1955},
  note         = {Publisher: [Oxford University Press, American Association for Public 
  Opinion Research]},
  keywords     = {Inter Rater Reliability}
}

@article{31,
  title        = {Measuring nominal scale agreement among many raters.},
  volume       = {76},
  issn         = {0033-2909},
  url          = {http://content.apa.org/journals/bul/76/5/378},
  doi          = {10.1037/h0031619},
  pages        = {378--382},
  number       = {5},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  author       = {Fleiss, Joseph L.},
  urldate      = {2020-12-29},
  date         = {1971},
  langid       = {english},
  keywords     = {Inter Rater Reliability}
}

@online{32,
  title    = {Computing inter‐rater reliability and its variance in the presence of high 
  agreement - Gwet - 2008 - British Journal of Mathematical and Statistical Psychology 
  - Wiley Online Library},
  url      = {https://bpspsychub.onlinelibrary.wiley.com/doi/full/10.1348/000711006X126600?
  casa_token=5iF-i8Qxc_MAAAAA%3AyH8jCJPXFxwcVx1sMkF-Y--pUY_OV6JtT_mrpWzuBArpfyrWidFIVq0WoSIEEa
  Z1ftUiczdqLTMkAvR9},
  urldate  = {2020-12-29},
  keywords = {Inter Rater Reliability}
}

@article{33,
  title        = {Evaluation: From Precision, Recall and F-Factor to {ROC}, Informedness, Markedness \& Correlation},
  volume       = {2},
  shorttitle   = {Evaluation},
  abstract     = {Commonly used evaluation measures including Recall, Precision, F-Factor and 
  Rand Accuracy are biased and should not be used without clear understanding of the biases, 
  and corresponding identification of chance or base case levels of the statistic. Using these 
  measures a system that performs worse in the objective sense of Informedness, can appear to 
  perform better under any of these commonly used measures. We discuss several concepts and 
  measures that reflect the probability that prediction is informed versus chance. 
  Informedness and introduce Markedness as a dual measure for the probability that prediction 
  is marked versus chance. Finally we demonstrate elegant connections between the concepts 
  of Informedness, Markedness, Correlation and Significance as well as their intuitive 
  relationships with Recall and Precision, and outline the extension from the dichotomous 
  case to the general multi-class case. .},
  journaltitle = {Mach. Learn. Technol.},
  shortjournal = {Mach. Learn. Technol.},
  author       = {Powers, David},
  date         = {2008-01-01},
  keywords     = {Inter Rater Reliability},
  annotation   = {Informedness}
}
  